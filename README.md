**AI Quality & Evaluation Engineer** | Building expertise in AI system reliability, safety, and trustworthiness

## ðŸŽ¯ What I'm Doing

I'm on a systematic journey to deepen my expertise in AI quality and evaluation, combining:
- **14 years at Microsoft** (Test â†’ SDET â†’ SDE)
- **MS in Computer Science** from Johns Hopkins University (2023)
- **Outlier (Scale AI)** â€” Expert evaluator for programming projects, ensuring high-quality outputs across C++, C#, Python, Java, and SQL
- **Hands-on AI tool evaluation** â€” documenting what works, what breaks, and why it matters

## ðŸ”¬ Current Focus

Systematically evaluating AI development tools through a quality engineering lens:
- Researching LLM-as-judge approachesâ€”testing prompt strategies, measuring human-AI agreement, identifying autograder blind spots
- Assessing AI assistants for reliability, safety, and real-world performance
- Documenting edge cases, failure modes, and unexpected behaviors

## ðŸ“Š What I'm Building Here

This GitHub tracks my AI evaluation work:
- `/nist-ai-rmf-evaluation` â€” Exploring how governance principles such as the NIST AI Risk Management Framework can mbe mapped to concrete, testable evaluation criteria for generative AI systems
- `/assistant-autograder` -  A learning project exploring how to use language models as judges for AI assistant responses.

## ðŸ”— Connect

- [LinkedIn](https://www.linkedin.com/in/rasiulyte/)
- [Portfolio](https://rasar.ai)

