**AI Quality & Evaluation Engineer** | Building expertise in AI system reliability, safety, and trustworthiness

## What I'm Doing

I'm on a systematic journey to deepen my expertise in AI quality and evaluation, combining:
- **14 years at Microsoft** (Test → SDET → SDE)
- **MS in Computer Science** from Johns Hopkins University (2023)
- **Outlier (Scale AI)** — Expert evaluator for programming projects, ensuring high-quality outputs across C++, C#, Python, Java, and SQL
- **Hands-on AI tool evaluation** — documenting what works, what breaks, and why it matters

## Current Focus

Systematically evaluating AI development tools through a quality engineering lens:
- Researching LLM-as-judge approaches—testing prompt strategies, measuring human-AI agreement, identifying autograder blind spots
- Assessing AI assistants for reliability, safety, and real-world performance
- Documenting edge cases, failure modes, and unexpected behaviors

## What I'm Building Here

This GitHub tracks my AI evaluation work:
- `/nist-ai-rmf-evaluation` — Exploring how governance principles such as the NIST AI Risk Management Framework can be mapped to concrete, testable evaluation criteria for generative AI systems
- `/assistant-autograder` -  A learning project exploring how to use language models as judges for AI assistant responses.
- `/ai-eval-monitoring` — System design for monitoring AI quality at production scale.


## Connect

- [LinkedIn](https://www.linkedin.com/in/rasiulyte/)
- [Portfolio](https://rasar.ai)

